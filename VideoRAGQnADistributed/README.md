# Video RAG Distributed 

## Introduction
This is a distributed version of VideoRAGQnQ with vector store & LLM inference separated as independent microservices. 

Video RAG is a framework that retrives video based on provided user prompt. It uses both video scene description generated by open source vision models (ex video-llama, video-llava etc.) as text embeddings and frames as image embeddings to perform vector similarity search. The provided solution also supports feature to retrieve more similar videos without prompting it. (see the example video below)

![Example Video](docs/visual-rag-demo.gif)

## Tools

- **UI**: streamlit
- **Vector Storage**: Chroma DB **or** Intel's VDMS
- **Image Embeddings**: CLIP
- **Text Embeddings**: all-MiniLM-L12-v2
- **RAG Retriever**: Langchain Ensemble Retrieval
- **Model server**: Ray serve with Llama2-7b

## Prerequisites

There are 10 example videos present in ```video_ingest/videos``` along with their description generated by open-source vision model.
If you want these video RAG to work on your own videos, make sure it matches below format.

## File Structure

```bash
video_ingest/
.
├── scene_description
│   ├── op_10_0320241830.mp4.txt
│   ├── op_1_0320241830.mp4.txt
│   ├── op_19_0320241830.mp4.txt
│   ├── op_21_0320241830.mp4.txt
│   ├── op_24_0320241830.mp4.txt
│   ├── op_31_0320241830.mp4.txt
│   ├── op_47_0320241830.mp4.txt
│   ├── op_5_0320241915.mp4.txt
│   ├── op_DSCF2862_Rendered_001.mp4.txt
│   └── op_DSCF2864_Rendered_006.mp4.txt
└── videos
    ├── op_10_0320241830.mp4
    ├── op_1_0320241830.mp4
    ├── op_19_0320241830.mp4
    ├── op_21_0320241830.mp4
    ├── op_24_0320241830.mp4
    ├── op_31_0320241830.mp4
    ├── op_47_0320241830.mp4
    ├── op_5_0320241915.mp4
    ├── op_DSCF2862_Rendered_001.mp4
    └── op_DSCF2864_Rendered_006.mp4
```

## Setup and Installation

Install pip requirements

```bash
cd VideoRAGQnADistributed
pip3 install -r docs/requirements.txt
```

```
### Vector store microservice setup

Please build and run the service as the following steps: 

```bash
cd VideoRAGQnADistributed/vectorDB_service
docker compose build
docker compose up
```

Currently ChromaDB and Intel's VDMS are supported.
For full API docs, please visit `http://server_ip:9001/docs` for automatic interactive API documentation
After the vectorDB service is alive, please update the following entries in `docs/config.yaml` with the actual ip addr of your service. 

```yaml
vector_query_url: http://<your_service_ip>:9001/video_llama_retriever/query
vector_init_url: http://<your_service_ip>:9001/video_llama_retriever/init_db
vector_health_url: http://<your_service_ip>:9001/health
image_insert_url: http://<your_service_ip>:9001/video_llama_retriever/upload_images
text_insert_url: http://<your_service_ip>:9001/video_llama_retriever/add_texts
```

To generate image embeddings and store them into selected db, specify the kind of db (chroma or VDMS) and video input location in docs/config.yaml and do:
```bash
python3 embedding/generate_store_embeddings.py docs/config.yaml video_ingest/videos/
```

### Model server microservice setup
Please first setup your huggingface hub API [token](https://huggingface.co/login?next=%2Fsettings%2Ftokens).
And download the `llama-2-7b-chat-hf` model file as following:
```bash
export HUGGINGFACEHUB_API_TOKEN='<your HF token>' 
git clone https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
```

Build and start the model server container as following:
```bash
cd VideoRAGQnADistributed/model_serve
docker compose build
docker compose up
```

**Web UI Video RAG**
```bash
streamlit run video-rag-ui.py --server.address 0.0.0.0 --server.port 50055
```
