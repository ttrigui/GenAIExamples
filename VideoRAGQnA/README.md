# Video RAG

## Introduction
Video RAG is a framework that retrieves video based on provided user prompt. It uses only the video embeddings to perform vector similarity search in Intel's VDMS vector database and performs all operations on Intel Xeon CPU. The pipeline supports long form videos and time-based search. The provided solution also supports feature to retrieve more similar videos without prompting it. (see the example video below)

![Example Video](docs/visual-rag-demo.gif)

## Tools

- **UI**: streamlit
- **Vector Storage**: Intel's VDMS
- **Video Embeddings**: CLIP w/ mean aggragation
- **RAG Retriever**: Langchain MultiModal Retrieval
- **LLM**: Llama-2-7b-chat-hf

## Prerequisites

There are 10 example videos present in ```video_ingest/videos``` along with their description generated by open-source vision model.
If you want these video RAG to work on your own videos, make sure it matches below format.

## File Structure

```bash
video_ingest/
.
└── videos
    ├── op_10_0320241830.mp4
    ├── op_1_0320241830.mp4
    ├── op_19_0320241830.mp4
    ├── op_21_0320241830.mp4
    ├── op_24_0320241830.mp4
    ├── op_31_0320241830.mp4
    ├── op_47_0320241830.mp4
    ├── op_5_0320241915.mp4
    ├── op_DSCF2862_Rendered_001.mp4
    └── op_DSCF2864_Rendered_006.mp4
```

## Setup and Installation

Install pip requirements

```bash
cd VideoRAGQnA
conda create --name vrag python=3.9 && conda activate vrag
pip install -r docs/requirements.txt
git clone https://github.com/langchain-ai/langchain
pip install -e langchain/libs/community
```

Install Intel's torch dependencies

``` 
pip install --pre --upgrade ipex-llm[all] --extra-index-url https://download.pytorch.org/whl/cpu
pip install torchvision==0.16.0+cpu  torchaudio==2.1.0+cpu --index-url https://download.pytorch.org/whl/cpu
```

Get access to gated meta-llama repo by requesting access here: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
Create HF access token by following these instructions: https://huggingface.co/docs/hub/en/security-tokens#how-to-manage-user-access-tokens

Finally, export your HF access token so the model weights can be pulled successfully before being converted to int4:
```
export HUGGINGFACEHUB_API_TOKEN="<YOUR_HF_TOKEN>"
```


Quantize and save Llama weights using Intel's Extension for PyTorch LLM
```
python int4_llama.py
```

Run start.sh script for fresh start. This will download model weights if not exist and restart VDMS container if it's left open. 
```bash
bash start.sh
```

After `start.sh`, if you stopped UI and want to reconnect to UI while keeping previously generated embeddings, execute `runUI.sh` to run UI app and connect to your app.
```bash
bash runUI.sh
```

Now you'll find UI up and running once you type `<ip.to.remote.machine>:50055` and hit enter in your brower's address bar.


**Note-1:** If you are not using file structure similar to what is described above, consider changing it in ```docs/config.yaml```.

**Note-2:** Update your choice of db and port in ```docs/config.yaml```.



