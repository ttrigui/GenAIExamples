# Video RAG

## Introduction
Video RAG is a framework that retrieves video based on provided user prompt. It uses only the video embeddings to perform vector similarity search in Intel's VDMS vector database and performs all operations on Intel Xeon CPU. The pipeline supports long form videos and time-based search. The provided solution also supports feature to retrieve more similar videos without prompting it. (see the example video below)

![Example Video](docs/visual-rag-demo.gif)

## Tools

- **UI**: streamlit
- **Vector Storage**: Intel's VDMS
- **Video Embeddings**: CLIP w/ mean aggragation
- **RAG Retriever**: Langchain Ensemble Retrieval
- **LLM**: Llama-2-7b-chat-hf

## Prerequisites

There are 10 example videos present in ```video_ingest/videos``` along with their description generated by open-source vision model.
If you want these visual RAG to work on your own videos, make sure it matches below format.

## File Structure

```bash
video_ingest/
.
└── videos
    ├── op_10_0320241830.mp4
    ├── op_1_0320241830.mp4
    ├── op_19_0320241830.mp4
    ├── op_21_0320241830.mp4
    ├── op_24_0320241830.mp4
    ├── op_31_0320241830.mp4
    ├── op_47_0320241830.mp4
    ├── op_5_0320241915.mp4
    ├── op_DSCF2862_Rendered_001.mp4
    └── op_DSCF2864_Rendered_006.mp4
```

## Setup and Installation

Install pip requirements

```bash
cd VideoRAGQnA
conda create --name vrag python=3.9 && conda activate vrag
pip install -r docs/requirements.txt
```

This code is using a pre-released update of vdms vectordb in LangChain.
```bash
cd cw-langchain-vdms-patch/libs/community
pip install -e .
cd ../../../
```

Download Llama-2-7b-chat-hf and video-llama models

```bash
mkdir -p embedding/video_llama_weights
cd embedding/video_llama_weights
wget https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-7B-Finetuned/resolve/main/VL_LLaMA_2_7B_Finetuned.pth

cd ../../
mkdir -p meta-llama/Llama-2-7b-chat-hf
cd meta-llama/Llama-2-7b-chat-hf
wget https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-7B-Finetuned/resolve/main/llama-2-7b-chat-hf/config.json
wget https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-7B-Finetuned/resolve/main/llama-2-7b-chat-hf/generation_config.json
wget https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-7B-Finetuned/resolve/main/llama-2-7b-chat-hf/pytorch_model-00001-of-00002.bin
wget https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-7B-Finetuned/resolve/main/llama-2-7b-chat-hf/pytorch_model-00002-of-00002.bin
wget https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-7B-Finetuned/resolve/main/llama-2-7b-chat-hf/pytorch_model.bin.index.json
wget https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-7B-Finetuned/resolve/main/llama-2-7b-chat-hf/special_tokens_map.json
wget https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-7B-Finetuned/resolve/main/llama-2-7b-chat-hf/tokenizer.json
wget https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-7B-Finetuned/resolve/main/llama-2-7b-chat-hf/tokenizer.model
wget https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-7B-Finetuned/resolve/main/llama-2-7b-chat-hf/tokenizer_config.json
cd ../..
```


The current framework supports Intel's VDMS.

Running VDMS DB as docker container
```bash
docker run -d -p 55555:55555 intellabs/vdms:latest
```

**Note-1:** If you are not using file structure similar to what is described above, consider changing it in ```docs/config.yaml```.

**Note-2:** Update your choice of db and port in ```docs/config.yaml```.


**Web UI Video RAG**
```bash
streamlit run video-rag-ui.py docs/config.yaml --server.address 0.0.0.0 --server.port 50055
```

